# Bandits
This package provide a framework for developing and comparing various Bandit algorithms

## Available Algorithms
1. ϵ-greedy
   1. ϵ-greedy(https://github.com/UmaArunachalam8/Bandits.jl/blob/master/doc/e-greedy.md)
   2. ϵ_n greedy
2. Upper Confidence Bound Policies
   1. UCB1
   2. UCB-Normal
   3. Discounted-UCB
   4. Sliding Window UCB
3. Thompson Sampling
   1. Thompson Sampling
   2. Dynamic Thompson Sampling
   3. Optimistic Thompson Sampling
4. EXP3
   1. EXP3
   2. EXP3.1
   3. EXP3-IX
5. SoftMax
6. REXP3

## Available Arm Models
1. Bernoulli
2. Beta
3. Normal
4. Sinusoidal (without noise)
5. Pulse (without noise)
6. Square
7. Variational (without noise)
